# è‡ªç„¶è¯­è¨€å¤„ç†:ç¬¬ä¸€ç™¾é›¶ä¹ç«  å•å¡4090å¾®è°ƒDeepSeek-R1-32B

**æœ¬äººé¡¹ç›®åœ°å€å¤§å…¨ï¼š[Victor94-king/NLP__ManVictor: CSDN of ManVictor](https://github.com/Victor94-king/NLP__ManVictor)**

<br />

***å†™åœ¨å‰é¢: ç¬”è€…æ›´æ–°ä¸æ˜“ï¼Œå¸Œæœ›èµ°è¿‡è·¯è¿‡ç‚¹ä¸ªå…³æ³¨å’Œèµï¼Œç¬”èŠ¯!!!***

***å†™åœ¨å‰é¢: ç¬”è€…æ›´æ–°ä¸æ˜“ï¼Œå¸Œæœ›èµ°è¿‡è·¯è¿‡ç‚¹ä¸ªå…³æ³¨å’Œèµï¼Œç¬”èŠ¯!!!***

***å†™åœ¨å‰é¢: ç¬”è€…æ›´æ–°ä¸æ˜“ï¼Œå¸Œæœ›èµ°è¿‡è·¯è¿‡ç‚¹ä¸ªå…³æ³¨å’Œèµï¼Œç¬”èŠ¯!!!***

<br />


åœ¨ 24G æ˜¾å­˜çš„å•å¡ 4090 ä¸Šå¾®è°ƒè®­ç»ƒ deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bï¼›å³ä½¿è¯¥æ¨¡å‹çš„æƒé‡æ–‡ä»¶å¤§å°å·²ç»è¾¾åˆ° 62Gï¼Œè¿™æ˜¯å› ä¸º unsloth å’Œ lora çš„é‡åŒ–å¾®è°ƒå’Œéƒ¨åˆ†å‚æ•°å¾®è°ƒä¼˜åŒ–å¯ä»¥å¤§å¹…èŠ‚çº¦æ˜¾å­˜å ç”¨ã€‚

å› ä¸ºè®¾ç½®äº†**max_steps=**60 é™åˆ¶äº†åªæ‰§è¡Œ60æ­¥ä»¥ä¾¿å¿«é€Ÿå®Œæˆå®éªŒã€‚å»æ‰è¿™ä¸ªå‚æ•°åï¼ŒSFTTrainer å³å¯æ ¹æ®æ•°æ®é‡è‡ªåŠ¨è®¡ç®—å¾®è°ƒæ­¥æ•°äº†ã€‚è¿™æ¬¡è¿›è¡Œäº† **FreedomIntelligence**/medical-o1-reasoning-SFT æ•°æ®é›† 24772 æ¡æ•°æ®çš„å…¨é‡å¾®è°ƒï¼Œepochä¸ºé»˜è®¤å€¼3ï¼Œå¾®è°ƒç»“æœå¦‚ä¸‹ï¼š

* **è®­ç»ƒæ€»æ­¥æ•° (Total steps) : 9288 æ­¥**
* **è®­ç»ƒæ€»è½®æ¬¡ (Epochs) : 3.0 è½® **
* **æ¯è½®æ•°æ®é‡: 24,772 æ¡æ•°æ®**
* **è®­ç»ƒæ—¶é—´: æ€»è®¡ 28å°æ—¶28åˆ†37ç§’ï¼ˆ102517.8411 ç§’ï¼‰**

æœ¬æ¬¡è®­ç»ƒåœ¨è´è”äº‘ç®—åŠ›å¹³å°( https://cloud.lccomputing.com )ä¸Šå®Œæˆã€‚

å®Œæ•´è®­ç»ƒä»£ç å¦‚ä¸‹ï¼š

```
importÂ wandb
# ç™»å½• wandb.ai ç”¨äºå®éªŒè·Ÿè¸ª
wandb.login(key="æ”¾ç½®ä½ çš„wandb.aiç½‘ç«™ä¸Šçš„token")
# åˆå§‹åŒ–wandbé¡¹ç›®
run = wandb.init(
Â  Â  project='Lora-R1-Distill-Qwen on Medical COT Dataset',
Â  Â  job_type="training",
Â  Â  anonymous="allow"
)


####################################################################################################
#Â 1.åŠ è½½æ¨¡å‹


# ä½¿ç”¨ unsloth ä¼˜åŒ–çš„ FastLanguageModel åŠ è½½æ¨¡å‹
from unslothÂ importÂ FastLanguageModel
max_seq_length =Â 4096Â # æœ€å¤§åºåˆ—é•¿åº¦
dtype = None Â  Â  Â  Â  Â # æ•°æ®ç±»å‹ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
load_in_4bit = True Â  # ä½¿ç”¨4bité‡åŒ–åŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜


# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
Â  Â  #model_name =Â "unsloth/DeepSeek-R1-Distill-Qwen-7B",
Â  Â  model_name =Â "/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
Â  Â  local_files_only=True, Â # é¿å…è”ç½‘
Â  Â  max_seq_length = max_seq_length,
Â  Â  dtype = dtype,
Â  Â  load_in_4bit = load_in_4bit,
Â  Â  #token = hf_token,Â 
)
print(model)


####################################################################################################
#Â 2.Â å®šä¹‰æç¤ºæ¨¡æ¿ï¼Œå¹¶åœ¨å¾®è°ƒå‰åšä¸€æ¬¡æ¨ç†


prompt_style =Â """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
Â  è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
Â  åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚
Â  ### Instruction:
Â  ä½ æ˜¯ä¸€ä½åœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„åŒ»å­¦ä¸“å®¶ã€‚
Â  è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚
Â  ### Question:
Â  {}
Â  ### Response:
Â  <think>{}"""
train_prompt_style = prompt_style +Â """
Â  </think>
Â  {}"""


# æµ‹è¯•ç”¨åŒ»å­¦é—®é¢˜
question =Â "ä¸€å70å²çš„ç”·æ€§æ‚£è€…å› èƒ¸ç—›ä¼´å‘•å16å°æ—¶å°±åŒ»ï¼Œå¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”STæ®µæŠ¬é«˜0.1~0.3mVï¼Œç»è¡¥æ¶²åè¡€å‹é™è‡³80/60mmHgï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§çš„ç—‡çŠ¶ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ"


# è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)Â 
inputs = tokenizer([prompt_style.format(question,Â "")], return_tensors="pt").to("cuda")


# ç”Ÿæˆå›ç­”
outputs = model.generate(
Â  Â  input_ids=inputs.input_ids,
Â  Â  attention_mask=inputs.attention_mask,
Â  Â  max_new_tokens=1200,
Â  Â  use_cache=True,
)
response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])


####################################################################################################
#Â 3.Â å¤„ç†æ•°æ®é›†


EOS_TOKEN = tokenizer.eos_token Â # æ·»åŠ ç»“æŸç¬¦æ ‡è®°
#æ ¼å¼åŒ–æç¤ºå‡½æ•°,ç”¨äºå¤„ç†æ•°æ®é›†ä¸­çš„ç¤ºä¾‹
def formatting_prompts_func(examples):
Â  Â  # ä»examplesä¸­æå–é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”
Â  Â  inputs = examples["Question"] Â  Â  Â # åŒ»å­¦é—®é¢˜åˆ—è¡¨
Â  Â  cots = examples["Complex_CoT"] Â  Â  # æ€ç»´é“¾åˆ—è¡¨Â 
Â  Â  outputs = examples["Response"] Â  Â  # å›ç­”åˆ—è¡¨
Â  Â Â 
Â  Â  # å­˜å‚¨æ ¼å¼åŒ–åçš„æ–‡æœ¬
Â  Â  texts = []
Â  Â Â 
Â  Â  # éå†æ¯ä¸ªç¤ºä¾‹,å°†é—®é¢˜ã€æ€ç»´é“¾å’Œå›ç­”ç»„åˆæˆæŒ‡å®šæ ¼å¼
Â  Â Â forÂ input, cot, output in zip(inputs, cots, outputs):
Â  Â  Â  Â  # ä½¿ç”¨train_prompt_styleæ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬,å¹¶æ·»åŠ ç»“æŸç¬¦
Â  Â  Â  Â  text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
Â  Â  Â  Â  texts.append(text)
Â  Â  Â  Â Â 
Â  Â  # è¿”å›æ ¼å¼åŒ–åçš„æ–‡æœ¬å­—å…¸
Â  Â Â returnÂ {
Â  Â  Â  Â Â "text": texts,
Â  Â  }


# åŠ è½½æ•°æ®é›†å¹¶åº”ç”¨æ ¼å¼åŒ–
from datasetsÂ importÂ load_dataset,load_from_disk
dataset = load_dataset(
Â  Â Â "json", Â # æŒ‡å®šæ•°æ®æ ¼å¼ä¸º JSON
Â  Â  data_files="/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/medical_o1_sft_Chinese.json",
Â  Â  #split="train[0:500]", Â # åªå–å‰Â 500Â æ¡æ•°æ®
Â  Â  trust_remote_code=True Â # å…¼å®¹ remote code çš„è¡Œä¸º
)


# å¦‚æœè¿”å›çš„æ˜¯ DatasetDictï¼Œåˆ™å–å‡ºÂ "train"Â è¿™ä¸€éƒ¨åˆ†
ifÂ isinstance(dataset, dict): Â 
Â  Â  dataset = dataset["train"]
Â  Â Â 
dataset = dataset.map(formatting_prompts_func, batched = True,)
print(dataset) Â # æŸ¥çœ‹æ•°æ®é›†ç»“æ„


####################################################################################################
#Â 4.Â é…ç½®è®­ç»ƒå‚æ•°å¯åŠ¨è®­ç»ƒ


model = FastLanguageModel.get_peft_model(
Â  Â  model,Â 
Â  Â  r=32,
Â  Â  target_modules=[
Â  Â  Â  Â Â "q_proj",Â "k_proj",Â "v_proj",Â "o_proj",Â 
Â  Â  Â  Â Â "gate_proj",Â "up_proj",Â "down_proj", Â  Â 
Â  Â  ],
Â  Â  lora_alpha=16,
Â  Â  lora_dropout=0, Â 
Â  Â  bias="none", Â 
Â  Â  use_gradient_checkpointing="unsloth",Â 
Â  Â  random_state=8137,
Â  Â  use_rslora=False, Â 
Â  Â  loftq_config=None,
)
print(model)


# é…ç½®è®­ç»ƒå‚æ•°å’Œåˆå§‹åŒ–è®­ç»ƒå™¨
from trlÂ importÂ SFTTrainer Â 
from transformersÂ importÂ TrainingArguments Â 
from unslothÂ importÂ is_bfloat16_supported Â 


# åˆå§‹åŒ– SFT è®­ç»ƒå™¨
trainer = SFTTrainer(
Â  Â  model=model, Â 
Â  Â  tokenizer=tokenizer, Â 
Â  Â  train_dataset=dataset, Â 
Â  Â  dataset_text_field="text", Â # æ•°æ®é›†å­—æ®µçš„åç§°
Â  Â  max_seq_length=max_seq_length, Â 
Â  Â  dataset_num_proc=2, Â # æ•°æ®é›†å¤„ç†çš„å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œæé«˜CPUåˆ©ç”¨ç‡
Â  Â  args=TrainingArguments(
Â  Â  Â  Â  per_device_train_batch_size=2,Â 
Â  Â  Â  Â  gradient_accumulation_steps=4, Â Â 
Â  Â  Â  Â  warmup_steps=5, Â # é¢„çƒ­æ­¥æ•°,é€æ­¥å¢åŠ å­¦ä¹ ç‡
Â  Â  Â  Â  learning_rate=2e-4, Â # å­¦ä¹ ç‡
Â  Â  Â  Â  lr_scheduler_type="linear", Â # çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
Â  Â  Â  Â  # max_steps=200, Â  Â # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆä¸€æ­¥ = å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®ï¼‰
Â  Â  Â  Â  fp16=not is_bfloat16_supported(), Â # å¦‚æœä¸æ”¯æŒbf16åˆ™ä½¿ç”¨fp16
Â  Â  Â  Â  bf16=is_bfloat16_supported(), Â  Â  Â # å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨bf16
Â  Â  Â  Â  logging_steps=10, Â # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
Â  Â  Â  Â  optim="adamw_8bit", Â # ä½¿ç”¨8ä½AdamWä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜ï¼Œå‡ ä¹ä¸å½±å“è®­ç»ƒæ•ˆæœ
Â  Â  Â  Â  weight_decay=0.01, Â  # æƒé‡è¡°å‡ç³»æ•°,ç”¨äºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
Â  Â  Â  Â  seed=8137, Â # éšæœºæ•°ç§å­
Â  Â  Â  Â  output_dir="outputs", Â # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹å’Œè®­ç»ƒæ—¥å¿—
Â  Â  Â  Â  run_name="medical-o1-sft-experiment", Â # æ˜¾å¼è®¾ç½® wandb è¿è¡Œåç§°ï¼Œé¿å…è­¦å‘Š
Â  Â  ),
)


# å¼€å§‹è®­ç»ƒ
print(f"trainer.args.max_steps: {trainer.args.max_steps}")
print(f"trainer.args.num_train_epochs: {trainer.args.num_train_epochs}")
trainer.train()
print(f"Total training steps: {trainer.state.max_steps}")
print(f"Total epochs: {trainer.state.epoch}")


####################################################################################################
#Â 5.Â å¾®è°ƒåçš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†


FastLanguageModel.for_inference(model) Â 
inputs = tokenizer([prompt_style.format(question,Â "")], return_tensors="pt").to("cuda")


# ç”Ÿæˆå›ç­”
outputs = model.generate(
Â  Â  input_ids=inputs.input_ids, # è¾“å…¥tokençš„idåºåˆ—
Â  Â  attention_mask=inputs.attention_mask, Â # æ³¨æ„åŠ›æ©ç ,ç”¨äºæ ‡è®°æœ‰æ•ˆè¾“å…¥ä½ç½®
Â  Â  max_new_tokens=1200, # ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°é‡
Â  Â  use_cache=True, # æ˜¯å¦ä½¿ç”¨KVç¼“å­˜åŠ é€Ÿç”Ÿæˆ
)


response = tokenizer.batch_decode(outputs)
print("### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1])


####################################################################################################
#Â 6.Â ä¿å­˜æ¨¡å‹


new_model_local =Â "DeepSeek-R1-Medical-COT-Qwen-32B"
model.save_pretrained(new_model_local)Â 
tokenizer.save_pretrained(new_model_local)


# ä¿å­˜åˆå¹¶åçš„16bitæ¨¡å‹
model.save_pretrained_merged(new_model_local, tokenizer, save_method =Â "merged_16bit",)


# ä¿å­˜ä¸º GGUF æ¨¡å‹
# model.save_pretrained_gguf("DeepSeek-R1-Qwen-32B-Medical-COT-GGUF", tokenizer,)
```

å®Œæ•´æ—¥å¿—å¦‚ä¸‹ï¼š

```
ounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(lineounter(line
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend. Â Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb versionÂ 0.19.6
wandb: Run data is saved locally in /workspace/wandb/run-20250212_150918-mvocwedu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-wind-2
wandb: â­ï¸ View project at https://wandb.ai/xlxkming-none/Lora-R1-Distill-Qwen%20on%20Medical%20COT%20Dataset?apiKey=edb4e5ad4f056c86bc64f3ea1d5b327e88378327
wandb: ğŸš€ View run at https://wandb.ai/xlxkming-none/Lora-R1-Distill-Qwen%20on%20Medical%20COT%20Dataset/runs/mvocwedu?apiKey=edb4e5ad4f056c86bc64f3ea1d5b327e88378327
wandb: WARNING Do NOT share these links with anyone. They can be used to claim your runs.
ğŸ¦¥ Unsloth: Will patch your computer to enableÂ 2x faster free finetuning.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFOÂ 02-12Â 15:09:30Â __init__.py:190] Automatically detected platform cuda.
==((====))== Â UnslothÂ 2025.2.4: Fast Qwen2 patching. Transformers:Â 4.48.3.
Â  Â \\ Â  /| Â  Â GPU: NVIDIA GeForce RTXÂ 4090. Max memory:Â 23.65Â GB. Platform: Linux.
O^O/ \_/ \ Â  Â Torch:Â 2.5.1+cu121. CUDA:Â 8.9. CUDA Toolkit:Â 12.1. Triton:Â 3.1.0
\ Â  Â  Â  Â / Â  Â Bfloat16 = TRUE. FA [Xformers =Â 0.0.29.post1. FA2 = False]
Â "-____-"Â  Â  Â Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:Â 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|Â 8/8Â [00:16<00:00, Â 2.07s/it]
UnslothÂ 2025.2.4Â patchedÂ 64Â layers withÂ 64Â QKV layers,Â 64Â O layers andÂ 64Â MLP layers.
/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B does not have a padding token! Will use pad_token = <|vision_pad|>.
Qwen2ForCausalLM(
Â  (model):Â Qwen2Model(
Â  Â  (embed_tokens):Â Embedding(152064,Â 5120, padding_idx=151654)
Â  Â  (layers):Â ModuleList(
Â  Â  Â  (0-63):Â 64Â xÂ Qwen2DecoderLayer(
Â  Â  Â  Â  (self_attn):Â Qwen2Attention(
Â  Â  Â  Â  Â  (q_proj):Â Linear4bit(in_features=5120, out_features=5120, bias=True)
Â  Â  Â  Â  Â  (k_proj):Â Linear4bit(in_features=5120, out_features=1024, bias=True)
Â  Â  Â  Â  Â  (v_proj):Â Linear4bit(in_features=5120, out_features=1024, bias=True)
Â  Â  Â  Â  Â  (o_proj):Â Linear4bit(in_features=5120, out_features=5120, bias=False)
Â  Â  Â  Â  Â  (rotary_emb):Â LlamaRotaryEmbedding()
Â  Â  Â  Â  )
Â  Â  Â  Â  (mlp):Â Qwen2MLP(
Â  Â  Â  Â  Â  (gate_proj):Â Linear4bit(in_features=5120, out_features=27648, bias=False)
Â  Â  Â  Â  Â  (up_proj):Â Linear4bit(in_features=5120, out_features=27648, bias=False)
Â  Â  Â  Â  Â  (down_proj):Â Linear4bit(in_features=27648, out_features=5120, bias=False)
Â  Â  Â  Â  Â  (act_fn):Â SiLU()
Â  Â  Â  Â  )
Â  Â  Â  Â  (input_layernorm):Â Qwen2RMSNorm((5120,), eps=1e-05)
Â  Â  Â  Â  (post_attention_layernorm):Â Qwen2RMSNorm((5120,), eps=1e-05)
Â  Â  Â  )
Â  Â  )
Â  Â  (norm):Â Qwen2RMSNorm((5120,), eps=1e-05)
Â  Â  (rotary_emb):Â LlamaRotaryEmbedding()
Â  )
Â  (lm_head):Â Linear(in_features=5120, out_features=152064, bias=False)
)
### å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š
Â  <think><think>
å—¯ï¼Œè¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œä½†æˆ‘ä¼šä¸€æ­¥ä¸€æ­¥æ¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£æ‚£è€…çš„æƒ…å†µå’Œæ£€æŸ¥ç»“æœï¼Œç„¶åç¡®å®šå¯èƒ½çš„è¯Šæ–­ï¼Œæœ€åé€‰æ‹©åˆé€‚çš„è¯ç‰©æ²»ç–—ã€‚
æ‚£è€…æ˜¯ä¸€ä½70å²çš„ç”·æ€§ï¼Œå› èƒ¸ç—›å’Œå‘•å16å°æ—¶æ¥å°±åŒ»ã€‚å¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”çš„STæ®µæŠ¬é«˜ï¼Œè¿™å¯èƒ½æç¤ºå¿ƒè‚Œæ¢—æ­»ï¼Œç‰¹åˆ«æ˜¯ä¸‹å£å’Œå³å®¤æ¢—æ­»ã€‚å› ä¸ºä¸‹å£å¿ƒè‚Œæ¢—æ­»é€šå¸¸ä¸å³å† çŠ¶åŠ¨è„‰é˜»å¡æœ‰å…³ï¼Œè€Œå³èƒ¸å¯¼è”çš„STæŠ¬é«˜å¯èƒ½æ¶‰åŠå³å¿ƒå®¤ã€‚
æ¥ä¸‹æ¥ï¼Œæ‚£è€…åœ¨è¡¥æ¶²åè¡€å‹é™è‡³80/60Â mmHgï¼Œè¿™å¯èƒ½æ„å‘³ç€ä½è¡€å‹ï¼Œä½†è¡¥æ¶²åè¡€å‹åè€Œä¸‹é™ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå¿ƒè„åŠŸèƒ½å—æŸï¼Œæ— æ³•æœ‰æ•ˆæ³µè¡€ï¼Œå¯¼è‡´å¿ƒæºæ€§ä¼‘å…‹ã€‚åŒæ—¶ï¼Œæ‚£è€…å‡ºç°å‘¼å¸å›°éš¾å’Œä¸èƒ½å¹³å§ï¼Œä½“æ£€å‘ç°åŒè‚ºæœ‰å¤§é‡æ°´æ³¡éŸ³ï¼Œè¿™å¯èƒ½æç¤ºè‚ºæ°´è‚¿ï¼Œå°¤å…¶æ˜¯å¿ƒæºæ€§è‚ºæ°´è‚¿ï¼Œå› ä¸ºå¿ƒè„æ— æ³•æœ‰æ•ˆæ³µè¡€ï¼Œå¯¼è‡´æ¶²ä½“ç§¯èšåœ¨è‚ºéƒ¨ã€‚
ç°åœ¨ï¼Œæˆ‘éœ€è¦ç¡®å®šæ‚£è€…çš„å…·ä½“æƒ…å†µã€‚ä¸‹å£å’Œå³å®¤æ¢—æ­»å¯èƒ½å¯¼è‡´å¿ƒè„æ³µè¡€åŠŸèƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯å³å¿ƒå®¤åŠŸèƒ½ä¸å…¨ï¼Œå½±å“å¿ƒè„çš„è¾“å‡ºï¼Œå¯¼è‡´ä½è¡€å‹å’Œè‚ºæ°´è‚¿ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…çš„è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€å¯èƒ½ä¸ç¨³å®šï¼Œéœ€è¦ç´§æ€¥å¤„ç†ã€‚
æ¥ä¸‹æ¥ï¼Œè€ƒè™‘è¯ç‰©æ²»ç–—ã€‚é€šå¸¸ï¼Œå¯¹äºå¿ƒè‚Œæ¢—æ­»ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨æŠ—è¡€å°æ¿è¯ç‰©ï¼ˆå¦‚é˜¿å¸åŒ¹æ—ï¼‰ã€æŠ—å‡è¯ç‰©ï¼ˆå¦‚è‚ç´ æˆ–æ›¿æ ¼ç‘æ´›ï¼‰ï¼Œä»¥åŠÎ²å—ä½“é˜»æ»å‰‚ã€ACEIæˆ–ARBç±»è¯ç‰©æ¥æ”¹å–„å¿ƒè„åŠŸèƒ½å’Œå‡å°‘å¿ƒè„è´Ÿè·ã€‚ç„¶è€Œï¼Œæ‚£è€…ç°åœ¨è¡€å‹ä½ï¼Œå¯èƒ½ä¸é€‚åˆä½¿ç”¨ACEIï¼Œå› ä¸ºACEIå¯èƒ½ä¼šè¿›ä¸€æ­¥é™ä½è¡€å‹ï¼Œå¯¼è‡´ä½è¡€å‹åŠ é‡ã€‚
æ­¤å¤–ï¼Œæ‚£è€…å‡ºç°è‚ºæ°´è‚¿ï¼Œå¯èƒ½éœ€è¦åˆ©å°¿å‰‚æ¥å‡è½»è‚ºéƒ¨æ¶²ä½“ç§¯èšã€‚ä½†åˆ©å°¿å‰‚å¯èƒ½ä¼šå¯¼è‡´è¡€å®¹é‡è¿›ä¸€æ­¥å‡å°‘ï¼Œä»è€ŒåŠ é‡ä½è¡€å‹ï¼Œè¿™å¯èƒ½ä¸å¤ªé€‚åˆå½“å‰çš„æƒ…å†µã€‚
è€ƒè™‘åˆ°æ‚£è€…çš„ä½è¡€å‹å’Œè‚ºæ°´è‚¿ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©ï¼Œå¦‚å¤šå·´èƒºæˆ–å¤šå·´é…šä¸èƒºï¼Œæ¥å¢å¼ºå¿ƒè„æ”¶ç¼©åŠ›ï¼Œæ”¹å–„å¿ƒè„è¾“å‡ºï¼Œä»è€Œæå‡è¡€å‹å’Œå‡è½»è‚ºæ°´è‚¿ã€‚åŒæ—¶ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å…¶ä»–è¯ç‰©çš„ä½¿ç”¨ï¼Œä»¥é¿å…è¿›ä¸€æ­¥å½±å“è¡€å‹ã€‚
å¦å¤–ï¼Œæ‚£è€…å¯èƒ½éœ€è¦æœºæ¢°é€šæ°”æ”¯æŒï¼Œç‰¹åˆ«æ˜¯å¦‚æœå‘¼å¸å›°éš¾ä¸¥é‡ï¼Œæ— æ³•å¹³å§ï¼Œå¯èƒ½éœ€è¦æ— åˆ›é€šæ°”æˆ–æ’ç®¡ã€‚ä½†è¿™å¯èƒ½è¶…å‡ºäº†å½“å‰è¯ç‰©å¤„ç†çš„èŒƒå›´ã€‚
ç»¼ä¸Šæ‰€è¿°ï¼Œæ‚£è€…çš„æƒ…å†µå¯èƒ½æ¶‰åŠä¸‹å£å’Œå³å®¤å¿ƒè‚Œæ¢—æ­»ï¼Œå¯¼è‡´å¿ƒæºæ€§ä¼‘å…‹å’Œè‚ºæ°´è‚¿ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†å¯èƒ½åŒ…æ‹¬ä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©ï¼ˆå¦‚å¤šå·´èƒºæˆ–å¤šå·´é…šä¸èƒºï¼‰æ¥æ”¹å–„å¿ƒè„åŠŸèƒ½ï¼ŒåŒæ—¶ç»§ç»­æŠ—è¡€å°æ¿å’ŒæŠ—å‡æ²»ç–—ï¼Œä½†éœ€è°¨æ…è°ƒæ•´ä»¥é¿å…ä½è¡€å‹åŠ é‡ã€‚å¯èƒ½è¿˜éœ€è¦åˆ©å°¿å‰‚æ¥å‡è½»è‚ºæ°´è‚¿ï¼Œä½†éœ€åœ¨ç›‘æµ‹ä¸‹ä½¿ç”¨ï¼Œä»¥é˜²æ­¢è¡€å®¹é‡è¿‡ä½ã€‚
å½“ç„¶ï¼Œå…·ä½“æƒ…å†µå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°ï¼Œå¦‚å¿ƒè„è¶…å£°æ£€æŸ¥ï¼Œä»¥ç¡®å®šå³å¿ƒå®¤çš„åŠŸèƒ½å’Œæ˜¯å¦å­˜åœ¨æœºæ¢°å¹¶å‘ç—‡ï¼Œå¦‚å®¤é—´éš”ç©¿å­”æˆ–ä¹³å¤´è‚ŒåŠŸèƒ½ä¸å…¨ã€‚æ­¤å¤–ï¼Œå¯èƒ½éœ€è¦ä»‹å…¥æ²»ç–—ï¼Œå¦‚å† çŠ¶åŠ¨è„‰é€ å½±å’Œæ”¯æ¶æ¤å…¥ï¼Œä»¥æ¢å¤è¡€æµï¼Œæ”¹å–„å¿ƒè„åŠŸèƒ½ã€‚
ä½†æ ¹æ®é—®é¢˜ï¼Œä¸»è¦æ˜¯åœ¨è¯ç‰©å¤„ç†æ–¹é¢ï¼Œå› æ­¤é‡ç‚¹åº”æ”¾åœ¨ä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©å’Œæ”¯æŒæ€§æ²»ç–—ä¸Šï¼ŒåŒæ—¶ç›‘æµ‹å’Œè°ƒæ•´å…¶ä»–è¯ç‰©çš„ä½¿ç”¨ã€‚
</think>
é’ˆå¯¹è¯¥æ‚£è€…çš„æƒ…å†µï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†å¦‚ä¸‹ï¼š
1. **æŠ—è¡€å°æ¿å’ŒæŠ—å‡æ²»ç–—**ï¼šç»§ç»­ä½¿ç”¨é˜¿å¸åŒ¹æ—å’Œæ°¯å¡æ ¼é›·ï¼ˆæˆ–æ›¿æ ¼ç‘æ´›ï¼‰ï¼Œå¹¶ç»™äºˆè‚ç´ æŠ—å‡ï¼Œä»¥é˜²æ­¢è¡€æ “è¿›ä¸€æ­¥å½¢æˆã€‚
2. **æ­£æ€§è‚ŒåŠ›è¯ç‰©**ï¼šä½¿ç”¨å¤šå·´èƒºæˆ–å¤šå·´é…šä¸èƒºï¼Œä»¥å¢å¼ºå¿ƒè„æ”¶ç¼©åŠ›ï¼Œæ”¹å–„å¿ƒè„è¾“å‡ºï¼Œæå‡è¡€å‹ï¼Œå¹¶å‡è½»è‚ºæ°´è‚¿ã€‚
3. **åˆ©å°¿å‰‚**ï¼šåœ¨ç›‘æµ‹ä¸‹ä½¿ç”¨åˆ©å°¿å‰‚ï¼ˆå¦‚å‘‹å¡ç±³ï¼‰ä»¥å‡è½»è‚ºæ°´è‚¿ï¼Œä½†éœ€æ³¨æ„é¿å…è¡€å®¹é‡è¿‡ä½ã€‚
4. **é¿å…ä½¿ç”¨ACEIæˆ–ARB**ï¼šç”±äºæ‚£è€…è¡€å‹ä½ï¼Œæš‚æ—¶é¿å…ä½¿ç”¨ACEIæˆ–ARBï¼Œä»¥é˜²æ­¢è¿›ä¸€æ­¥é™ä½è¡€å‹ã€‚
5. **ç›‘æµ‹å’Œæ”¯æŒæ²»ç–—**ï¼šå¯†åˆ‡ç›‘æµ‹æ‚£è€…çš„ç”Ÿå‘½ä½“å¾ï¼Œå¿…è¦æ—¶è¿›è¡Œæœºæ¢°é€šæ°”æ”¯æŒï¼Œå¹¶è€ƒè™‘ä»‹å…¥æ²»ç–—ï¼ˆå¦‚å† çŠ¶åŠ¨è„‰é€ å½±å’Œæ”¯æ¶æ¤å…¥ï¼‰ä»¥æ¢å¤è¡€æµã€‚
ç»¼ä¸Šæ‰€è¿°ï¼Œè¯ç‰©å¤„ç†çš„é‡ç‚¹åœ¨äºä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©å’Œæ”¯æŒæ€§æ²»ç–—ï¼ŒåŒæ—¶ç»§ç»­æŠ—è¡€å°æ¿å’ŒæŠ—å‡æ²»ç–—ï¼Œä»¥æ”¹å–„å¿ƒè„åŠŸèƒ½å’Œè¡€æµåŠ¨åŠ›å­¦çŠ¶æ€ã€‚<ï½œendâ–ofâ–sentenceï½œ>
Dataset({
Â  Â  features: ['Question',Â 'Complex_CoT',Â 'Response',Â 'text'],
Â  Â  num_rows:Â 24772
})
PeftModelForCausalLM(
Â  (base_model):Â LoraModel(
Â  Â  (model):Â Qwen2ForCausalLM(
Â  Â  Â  (model):Â Qwen2Model(
Â  Â  Â  Â  (embed_tokens):Â Embedding(152064,Â 5120, padding_idx=151654)
Â  Â  Â  Â  (layers):Â ModuleList(
Â  Â  Â  Â  Â  (0-63):Â 64Â xÂ Qwen2DecoderLayer(
Â  Â  Â  Â  Â  Â  (self_attn):Â Qwen2Attention(
Â  Â  Â  Â  Â  Â  Â  (q_proj): lora.Linear4bit(
Â  Â  Â  Â  Â  Â  Â  Â  (base_layer):Â Linear4bit(in_features=5120, out_features=5120, bias=True)
Â  Â  Â  Â  Â  Â  Â  Â  (lora_dropout):Â ModuleDict(
Â  Â  Â  Â  Â  Â  Â  Â  Â  (default):Â Identity()
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  (lora_A):Â ModuleDict(
Â  Â  Â  Â  Â  Â  Â  Â  Â  (default):Â Linear(in_features=5120, out_features=32, bias=False)
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  (lora_B):Â ModuleDict(
Â  Â  Â  Â  Â  Â  Â  Â  Â  (default):Â Linear(in_features=32, out_features=5120, bias=False)
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  (lora_embedding_A):Â ParameterDict()
Â  Â  Â  Â  Â  Â  Â  Â  (lora_embedding_B):Â ParameterDict()
Â  Â  Â  Â  Â  Â  Â  Â  (lora_magnitude_vector):Â ModuleDict()
Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â ã€‚ã€‚ã€‚
)
...
```

è®­ç»ƒè¿‡ç¨‹å’Œç»“æœæ—¥å¿—ï¼š

```
==((====))== Â Unsloth - 2x faster free finetuning | Num GPUs = 1
Â  Â \\ Â  /| Â  Â Num examples = 24,772 | Num Epochs = 3
O^O/ \_/ \ Â  Â Batch size per device = 2 | Gradient Accumulation steps = 4
\ Â  Â  Â  Â / Â  Â Total batch size = 8 | Total steps = 9,288
Â "-____-"Â  Â  Â Number of trainable parameters = 268,435,456


trainer.args.max_steps: -1
trainer.args.num_train_epochs: 3.0
{'loss': 2.149,Â 'grad_norm': 0.16384364664554596,Â 'learning_rate': 0.00019989227620381345,Â 'epoch': 0.0}
{'loss': 1.5362,Â 'grad_norm': 0.07211203873157501,Â 'learning_rate': 0.0001996768286114403,Â 'epoch': 0.01}
{'loss': 1.4647,Â 'grad_norm': 0.07446285337209702,Â 'learning_rate': 0.00019946138101906713,Â 'epoch': 0.01}
...
{'loss': 1.39,Â 'grad_norm': 0.08653779327869415,Â 'learning_rate': 0.0001977378002800819,Â 'epoch': 0.04}
...
{'loss': 1.2627,Â 'grad_norm': 0.1181635782122612,Â 'learning_rate': 0.00013590434126898633,Â 'epoch': 0.96}
...
{'loss': 1.1951,Â 'grad_norm': 0.11674296855926514,Â 'learning_rate': 0.00013224173219864268,Â 'epoch': 1.02}
...
{'loss': 1.071,Â 'grad_norm': 0.1962611824274063,Â 'learning_rate': 3.1843154152752344e-05,Â 'epoch': 2.52}
...
{'loss': 0.9945,Â 'grad_norm': 0.17683860659599304,Â 'learning_rate': 2.2794355273079824e-05,Â 'epoch': 2.66}
...
{'loss': 1.1104,Â 'grad_norm': 0.21208912134170532,Â 'learning_rate': 6.032532586448347e-07,Â 'epoch': 2.99}
{'loss': 1.0957,Â 'grad_norm': 0.2164667695760727,Â 'learning_rate': 3.8780566627167944e-07,Â 'epoch': 2.99}
{'loss': 1.101,Â 'grad_norm': 0.21290326118469238,Â 'learning_rate': 1.723580738985242e-07,Â 'epoch': 3.0}


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9288/9288 [28:28:37<00:00, 11.04s/it]


{'train_runtime': 102517.8411,Â 'train_samples_per_second': 0.725,Â 'train_steps_per_second': 0.091,Â 'train_loss': 1.210533706973484,Â 'epoch': 3.0}
Total training steps: 9288
Total epochs: 2.999192636848054
### å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š


Â  <think>æ‚£è€…æ˜¯ä¸ª70å²çš„ç”·æ€§ï¼Œä»–å› ä¸ºèƒ¸ç—›å’Œå‘•åæ¥åˆ°åŒ»é™¢ï¼Œè¿™è®©æˆ‘é¦–å…ˆæƒ³åˆ°å¯èƒ½å’Œå¿ƒè„æœ‰å…³ã€‚å¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”çš„STæ®µæŠ¬é«˜ï¼Œå—¯ï¼Œè¿™å¯èƒ½æ„å‘³ç€ä¸‹å£å¿ƒè‚Œæ¢—æ­»ã€‚æ¥ä¸‹æ¥ï¼Œä»–çš„è¡€å‹åœ¨è¡¥æ¶²åé™åˆ°äº†80/60mmHgï¼Œè¿™å¾ˆä½ï¼Œè€Œä¸”ä»–è¿˜å‡ºç°äº†å‘¼å¸å›°éš¾ï¼Œä¸èƒ½å¹³å§ï¼Œè‚ºé‡Œæœ‰æ°´æ³¡éŸ³ï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æ€€ç–‘ä»–æœ‰å¿ƒæºæ€§ä¼‘å…‹æˆ–è€…æ€¥æ€§å¿ƒè¡°ã€‚


å¿ƒæºæ€§ä¼‘å…‹å’Œæ€¥æ€§å¿ƒè¡°é€šå¸¸éœ€è¦å¿«é€Ÿå¤„ç†ï¼Œå› ä¸ºè¿™ä¼šå±åŠç”Ÿå‘½ã€‚é¦–å…ˆæƒ³åˆ°çš„æ˜¯è¦ç¨³å®šä»–çš„è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€ã€‚é€šå¸¸ï¼Œè¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¼šä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©ï¼Œæ¯”å¦‚å¤šå·´é…šä¸èƒºï¼Œå› ä¸ºå®ƒå¯ä»¥å¢åŠ å¿ƒè‚Œæ”¶ç¼©åŠ›ï¼Œæé«˜å¿ƒè¾“å‡ºé‡ï¼Œå¸®åŠ©æ”¹å–„ä½è¡€å‹å’Œå‘¼å¸å›°éš¾çš„ç—‡çŠ¶ã€‚


ä¸è¿‡ï¼Œç­‰ç­‰ï¼Œæ‚£è€…æœ‰ä½è¡€å‹å’Œè‚ºæ°´è‚¿ï¼Œè¿™è®©æˆ‘è§‰å¾—å¯èƒ½ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•çº¯çš„å¿ƒæºæ€§ä¼‘å…‹ï¼Œè¿˜å¯èƒ½æœ‰æ¶²ä½“è¿‡è½½çš„é—®é¢˜ã€‚å¦‚æœæ˜¯æ¶²ä½“è¿‡å¤šï¼Œä½¿ç”¨åˆ©å°¿å‰‚å¦‚å‘‹å¡ç±³å¯èƒ½æ›´æœ‰æ•ˆï¼Œå› ä¸ºå®ƒèƒ½å¸®åŠ©æ’é™¤å¤šä½™çš„æ¶²ä½“ï¼Œå‡è½»è‚ºæ°´è‚¿ï¼ŒåŒæ—¶é™ä½å¿ƒè„çš„è´Ÿæ‹…ã€‚


å†æƒ³æƒ³ï¼Œæ‚£è€…çš„ä½è¡€å‹æƒ…å†µå¾ˆä¸¥é‡ï¼Œè€Œä¸”ä¸èƒ½å¹³å§ï¼Œè¿™å¯èƒ½æç¤ºå¿ƒè„æ³µåŠŸèƒ½éå¸¸å·®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©æ¥æå‡å¿ƒè„æ”¶ç¼©åŠ›å¯èƒ½æ›´åˆé€‚ã€‚å¤šå·´é…šä¸èƒºå¯ä»¥å¢åŠ å¿ƒè¾“å‡ºé‡ï¼ŒåŒæ—¶æ”¹å–„ä½è¡€å‹ï¼Œè¿™å¯èƒ½æ˜¯ç›®å‰æ›´å¥½çš„é€‰æ‹©ã€‚


å“¦ï¼Œå¯¹äº†ï¼Œæ‚£è€…çš„å¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£STæ®µæŠ¬é«˜ï¼Œè¿™å¯èƒ½æç¤ºå³å¿ƒå®¤æ¢—æ­»ã€‚å³å¿ƒå®¤æ¢—æ­»å¯èƒ½å¯¼è‡´å¿ƒæºæ€§ä¼‘å…‹ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šå·´é…šä¸èƒºæ¥å¢å¼ºå¿ƒè„æ”¶ç¼©åŠ›å’Œæé«˜å¿ƒè¾“å‡ºé‡å¯èƒ½æ˜¯æ›´åˆé€‚çš„ã€‚


ç»¼ä¸Šæ‰€è¿°ï¼Œè€ƒè™‘åˆ°æ‚£è€…ä¸¥é‡çš„ä½è¡€å‹ã€å‘¼å¸å›°éš¾å’Œå³å¿ƒå®¤æ¢—æ­»çš„å¯èƒ½æ€§ï¼Œä½¿ç”¨å¤šå·´é…šä¸èƒºæ¥è¿…é€Ÿæ”¹å–„è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€æ˜¯æœ€æ°å½“çš„ã€‚å—¯ï¼Œè¿™åº”è¯¥æ˜¯ä¸€ä¸ªæ˜æ™ºçš„é€‰æ‹©ã€‚
Â  </think>
Â  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…è¡¨ç°å‡ºä½è¡€å‹ã€å‘¼å¸å›°éš¾ã€ä¸èƒ½å¹³å§ä»¥åŠè‚ºéƒ¨æœ‰æ°´æ³¡éŸ³ï¼Œè¿™äº›ç—‡çŠ¶æç¤ºå¯èƒ½å­˜åœ¨å¿ƒæºæ€§ä¼‘å…‹æˆ–æ€¥æ€§å¿ƒåŠ›è¡°ç«­ã€‚å¿ƒç”µå›¾æ˜¾ç¤ºä¸‹å£å¯¼è”å’Œå³èƒ¸å¯¼è”çš„STæ®µæŠ¬é«˜ï¼Œæç¤ºå¯èƒ½æœ‰ä¸‹å£å¿ƒè‚Œæ¢—æ­»ï¼Œç”šè‡³å¯èƒ½æ¶‰åŠå³å¿ƒå®¤æ¢—æ­»ã€‚


å¯¹äºè¿™ç§æƒ…å†µï¼Œæœ€æ°å½“çš„è¯ç‰©å¤„ç†æ˜¯ä½¿ç”¨æ­£æ€§è‚ŒåŠ›è¯ç‰©æ¥æ”¹å–„å¿ƒè„çš„æ³µåŠŸèƒ½ï¼Œæé«˜å¿ƒè¾“å‡ºé‡ï¼Œä»è€Œæ”¹å–„ä½è¡€å‹å’Œå‘¼å¸å›°éš¾çš„ç—‡çŠ¶ã€‚å¤šå·´é…šä¸èƒºï¼ˆdobutamineï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ­£æ€§è‚ŒåŠ›è¯ç‰©ï¼Œå¯ä»¥å¢åŠ å¿ƒè‚Œæ”¶ç¼©åŠ›ï¼Œæé«˜å¿ƒè¾“å‡ºé‡ï¼ŒåŒæ—¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ‰©å¼ è¡€ç®¡ï¼Œé™ä½å¿ƒè„åè´Ÿè·ï¼Œæœ‰åŠ©äºæ”¹å–„æ‚£è€…çš„è¡€æµåŠ¨åŠ›å­¦çŠ¶æ€ã€‚


å› æ­¤ï¼Œè€ƒè™‘åˆ°æ‚£è€…ç›®å‰çš„è¡€æµåŠ¨åŠ›å­¦ä¸ç¨³å®šå’Œå¯èƒ½çš„å³å¿ƒå®¤æ¢—æ­»ï¼Œä½¿ç”¨å¤šå·´é…šä¸èƒºæ˜¯åˆç†ä¸”å¿…è¦çš„é€‰æ‹©ã€‚è¿™ç§è¯ç‰©å¹²é¢„å¯ä»¥è¿…é€Ÿå¸®åŠ©ç¨³å®šæ‚£è€…çš„ç—…æƒ…ï¼Œä¸ºåç»­çš„æ²»ç–—äº‰å–æ—¶é—´ã€‚<ï½œendâ–ofâ–sentenceï½œ>
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 303.83 out of 503.72 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Â  0%| Â  Â  Â  Â  Â | 0/64 [00:00<?, ?it/s]
We will save to Disk and not RAM now.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [01:34<00:00, Â 1.47s/it]
Unsloth: Saving tokenizer... Done.
Done.
wandb:
wandb: ğŸš€ View run ruby-wind-2 at: https://wandb.ai/xlxkming-none/Lora-R1-Distill-Qwen%20on%20Medical%20COT%20Dataset/runs/mvocwedu?apiKey=edb4e5ad4f056c86bc64f3ea1d5b327e88378327
wandb: Find logs at: wandb/run-20250212_150918-mvocwedu/logs
```

å³°å€¼èµ„æºå ç”¨ã€‚è¿™æ—¶è¿™æ¬¡ lora rank 32çš„ï¼š

```
ounter(lineounter(lineounter(lineounter(lineounter(line
|=========================================+======================+======================|
|Â  Â 0Â  NVIDIA GeForce RTXÂ 4090Â  Â  Â  Â  OffÂ |Â 00000000:01:00.0Â OffÂ |Â  Â  Â  Â  Â  Â  Â  Â  Â  OffÂ |
|Â 76%Â  Â 64C Â  Â P2 Â  Â  Â  Â  Â  Â Â 392WÂ /Â 450WÂ |Â Â 24176MiBÂ /Â 24564MiBÂ |Â  Â Â 100%Â  Â  Â Â DefaultÂ |
|Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â |Â  Â  Â  Â  Â  Â  Â  Â  Â  N/AÂ |
+-----------------------------------------+----------------------+----------------------+
```

ä¹‹å‰ lora rank 8 ä¹Ÿæµ‹è¿‡ï¼Œå’Œ rank 32 æ¯”èµ„æºå ç”¨å¹¶æ²¡å°‘å¤šå°‘ï¼š

```
ounter(lineounter(lineounter(lineounter(lineounter(line
|=========================================+======================+======================|
|Â  Â 0Â  NVIDIA GeForce RTXÂ 4090Â  Â  Â  Â  OffÂ |Â 00000000:01:00.0Â OffÂ |Â  Â  Â  Â  Â  Â  Â  Â  Â  OffÂ |
|Â 82%Â  Â 65C Â  Â P2 Â  Â  Â  Â  Â  Â Â 394WÂ /Â 450WÂ |Â Â 21246MiBÂ /Â 24564MiBÂ |Â  Â Â 100%Â  Â  Â Â DefaultÂ |
|Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â |Â  Â  Â  Â  Â  Â  Â  Â  Â  N/AÂ |
+-----------------------------------------+----------------------+-------------
```
